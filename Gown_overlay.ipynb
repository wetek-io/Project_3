{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: filelock in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## VITON-HD Virtual Try-On Model - Jupyter Notebook\n",
    "\n",
    "### **1. Setup the Environment**\n",
    "\n",
    "# Install dependencies\n",
    "%pip install torch torchvision numpy matplotlib opencv-python\n",
    "\n",
    "# Note: Creating and activating a virtual environment should be done in a terminal, not in a Jupyter Notebook cell.\n",
    "# The following commands are for reference and should be run in a terminal:\n",
    "# !conda create -n viton_hd python=3.7 -y\n",
    "# !conda activate viton_hd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person image not found. Using dummy pose and segmentation maps.\n",
      "Clothing image not found. Using a dummy tensor.\n"
     ]
    }
   ],
   "source": [
    "### **2. Preprocess Data**\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# Dummy functions for pose estimation and segmentation\n",
    "def estimate_pose(image):\n",
    "    # Dummy implementation for pose estimation\n",
    "    return \"pose_map\"\n",
    "\n",
    "def segment_person(image):\n",
    "    # Dummy implementation for person segmentation\n",
    "    return \"segmentation_map\"\n",
    "\n",
    "# Function to preprocess person images\n",
    "def preprocess_person(image_path):\n",
    "    person_image = cv2.imread(image_path)\n",
    "    if person_image is None:\n",
    "        raise FileNotFoundError(f\"Person image not found at path: {image_path}\")\n",
    "    pose_map = estimate_pose(person_image)  # Pose estimation function\n",
    "    segmentation_map = segment_person(person_image)  # Segmentation function\n",
    "    return pose_map, segmentation_map\n",
    "\n",
    "# Function to preprocess clothing images\n",
    "def preprocess_clothing(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((1024, 768)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    clothing_image = cv2.imread(image_path)\n",
    "    if clothing_image is None:\n",
    "        raise FileNotFoundError(f\"Clothing image not found at path: {image_path}\")\n",
    "    clothing_image = transform(clothing_image)\n",
    "    return clothing_image\n",
    "\n",
    "try:\n",
    "    pose_map, segmentation_map = preprocess_person(\"person.jpg\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Person image not found. Using dummy pose and segmentation maps.\")\n",
    "    pose_map, segmentation_map = \"pose_map\", \"segmentation_map\"\n",
    "\n",
    "try:\n",
    "    clothing_image = preprocess_clothing(\"clothing.jpg\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Clothing image not found. Using a dummy tensor.\")\n",
    "    clothing_image = torch.zeros((3, 1024, 768))  # Dummy tensor with the same shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from scikit-image) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from scikit-image) (1.13.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.1 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from scikit-image) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from scikit-image) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "### **3. Clothing Image Warping (Align Clothes to the Person)**\n",
    "%pip install scikit-image\n",
    "\n",
    "import numpy as np\n",
    "from skimage.transform import PiecewiseAffineTransform, warp\n",
    "\n",
    "# Warping clothing to align with the personâ€™s shape\n",
    "tform = PiecewiseAffineTransform()\n",
    "\n",
    "# Assuming pose_map is a set of control points for the transformation\n",
    "# You need to define source and destination control points for the transformation\n",
    "# Here we use dummy control points for illustration\n",
    "src_cols = np.linspace(0, clothing_image.shape[2], 10)\n",
    "src_rows = np.linspace(0, clothing_image.shape[1], 10)\n",
    "src_rows, src_cols = np.meshgrid(src_rows, src_cols)\n",
    "src = np.dstack([src_cols.flat, src_rows.flat])[0]\n",
    "\n",
    "dst = src + np.random.normal(0, 5, src.shape)\n",
    "tform.estimate(src, dst)\n",
    "\n",
    "warped_clothing = warp(clothing_image.permute(1, 2, 0).numpy(), tform)\n",
    "warped_clothing = torch.tensor(warped_clothing).permute(2, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement alias-generator (from versions: none)\n",
      "ERROR: No matching distribution found for alias-generator\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'alias_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### **4. Generate Try-On Image with ALIAS Normalization**\u001b[39;00m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall alias-generator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malias_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ALIASGenerator\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Using the ALIAS Generator to combine warped clothing and person image\u001b[39;00m\n\u001b[0;32m      7\u001b[0m alias_generator \u001b[38;5;241m=\u001b[39m ALIASGenerator()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'alias_generator'"
     ]
    }
   ],
   "source": [
    "### **4. Generate Try-On Image with ALIAS Normalization**\n",
    "%pip install alias-generator\n",
    "\n",
    "from alias_generator import ALIASGenerator\n",
    "\n",
    "# Using the ALIAS Generator to combine warped clothing and person image\n",
    "alias_generator = ALIASGenerator()\n",
    "output_image = alias_generator(warped_clothing, segmentation_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     44\u001b[0m     person_data, clothing_data \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 45\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mtryon_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclothing_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperson_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     loss \u001b[38;5;241m=\u001b[39m adversarial_loss(output, ground_truth) \u001b[38;5;241m+\u001b[39m perceptual_loss(output, ground_truth)\n\u001b[0;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[35], line 24\u001b[0m, in \u001b[0;36mTryOnModel.forward\u001b[1;34m(self, clothing_data, person_data)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, clothing_data, person_data):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Example forward pass\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclothing_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperson_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got str"
     ]
    }
   ],
   "source": [
    "### **5. Train the Model (if needed)**\n",
    "\n",
    "import torch\n",
    "\n",
    "# Define dummy dataloader, adversarial_loss, perceptual_loss, and ground_truth for the example\n",
    "dataloader = [(\"person_data\", clothing_image)]  # Dummy dataloader\n",
    "adversarial_loss = lambda output, target: torch.tensor(0.0)  # Dummy loss function\n",
    "perceptual_loss = lambda output, target: torch.tensor(0.0)  # Dummy loss function\n",
    "ground_truth = torch.zeros_like(clothing_image)  # Dummy ground truth\n",
    "\n",
    "# Define the tryon_model architecture\n",
    "class TryOnModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TryOnModel, self).__init__()\n",
    "        # Example model architecture with convolutional layers\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=6, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc = torch.nn.Linear(in_features=512*16*12, out_features=3*64*48)\n",
    "\n",
    "    def forward(self, clothing_data, person_data):\n",
    "        # Example forward pass\n",
    "        x = torch.cat((clothing_data, person_data), dim=1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = torch.sigmoid(self.fc(x))\n",
    "        output = output.view(-1, 3, 64, 48)\n",
    "        return output\n",
    "\n",
    "# Initialize the tryon_model\n",
    "tryon_model = TryOnModel()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(tryon_model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        person_data, clothing_data = batch\n",
    "        output = tryon_model(clothing_data, person_data)\n",
    "        loss = adversarial_loss(output, ground_truth) + perceptual_loss(output, ground_truth)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights file not found. Using dummy weights for testing.\n",
      "warped_clothing is not defined. Please ensure the cell generating warped_clothing is executed.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Generate try-on result\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 22\u001b[0m     output_image \u001b[38;5;241m=\u001b[39m \u001b[43mtryon_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwarped_clothing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegmentation_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kerim\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[35], line 24\u001b[0m, in \u001b[0;36mTryOnModel.forward\u001b[1;34m(self, clothing_data, person_data)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, clothing_data, person_data):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Example forward pass\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclothing_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperson_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got str"
     ]
    }
   ],
   "source": [
    "### **6. Run Inference**\n",
    "\n",
    "# Load trained model\n",
    "try:\n",
    "    tryon_model.load_state_dict(torch.load(\"viton_hd_weights.pth\"))\n",
    "    tryon_model.eval()\n",
    "except FileNotFoundError:\n",
    "    print(\"Weights file not found. Using dummy weights for testing.\")\n",
    "    dummy_state_dict = tryon_model.state_dict()\n",
    "    tryon_model.load_state_dict(dummy_state_dict)\n",
    "\n",
    "# Ensure warped_clothing is defined\n",
    "try:\n",
    "    warped_clothing\n",
    "except NameError:\n",
    "    print(\"warped_clothing is not defined. Please ensure the cell generating warped_clothing is executed.\")\n",
    "    # Dummy warped_clothing for testing\n",
    "    warped_clothing = torch.zeros_like(clothing_image)\n",
    "\n",
    "# Generate try-on result\n",
    "with torch.no_grad():\n",
    "    output_image = tryon_model(warped_clothing, segmentation_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_image is not defined. Please ensure the cell generating output_image is executed.\n"
     ]
    }
   ],
   "source": [
    "### **7. Display Results**\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the output_image is defined\n",
    "try:\n",
    "\tplt.imshow(output_image.permute(1, 2, 0))\n",
    "\tplt.axis(\"off\")\n",
    "\tplt.show()\n",
    "except NameError:\n",
    "\tprint(\"output_image is not defined. Please ensure the cell generating output_image is executed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
