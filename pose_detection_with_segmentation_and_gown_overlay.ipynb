{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the OpenPose model\n",
    "net = cv.dnn.readNetFromTensorflow(\"human-pose-estimation-opencv-master/graph_opt.pb\")\n",
    "\n",
    "# Input dimensions and confidence threshold\n",
    "inWidth = 368\n",
    "inHeight = 368\n",
    "thr = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "img = cv.imread(\"pose1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the image\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the body parts and the pairs of body parts to connect\n",
    "\n",
    "BODY_PARTS = {\n",
    "    \"Nose\": 0,\n",
    "    \"Neck\": 1,\n",
    "    \"RShoulder\": 2,\n",
    "    \"RElbow\": 3,\n",
    "    \"RWrist\": 4,\n",
    "    \"LShoulder\": 5,\n",
    "    \"LElbow\": 6,\n",
    "    \"LWrist\": 7,\n",
    "    \"RHip\": 8,\n",
    "    \"RKnee\": 9,\n",
    "    \"RAnkle\": 10,\n",
    "    \"LHip\": 11,\n",
    "    \"LKnee\": 12,\n",
    "    \"LAnkle\": 13,\n",
    "    \"REye\": 14,\n",
    "    \"LEye\": 15,\n",
    "    \"REar\": 16,\n",
    "    \"LEar\": 17,\n",
    "    \"Background\": 18,\n",
    "}\n",
    "\n",
    "POSE_PAIRS = [\n",
    "    [\"Neck\", \"RShoulder\"],\n",
    "    [\"Neck\", \"LShoulder\"],\n",
    "    [\"RShoulder\", \"RElbow\"],\n",
    "    [\"RElbow\", \"RWrist\"],\n",
    "    [\"LShoulder\", \"LElbow\"],\n",
    "    [\"LElbow\", \"LWrist\"],\n",
    "    [\"Neck\", \"RHip\"],\n",
    "    [\"RHip\", \"RKnee\"],\n",
    "    [\"RKnee\", \"RAnkle\"],\n",
    "    [\"Neck\", \"LHip\"],\n",
    "    [\"LHip\", \"LKnee\"],\n",
    "    [\"LKnee\", \"LAnkle\"],\n",
    "    [\"Neck\", \"Nose\"],\n",
    "    [\"Nose\", \"REye\"],\n",
    "    [\"REye\", \"REar\"],\n",
    "    [\"Nose\", \"LEye\"],\n",
    "    [\"LEye\", \"LEar\"],\n",
    "]\n",
    "\n",
    "\n",
    "def pose_estimation(frame):\n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "    net.setInput(\n",
    "        cv.dnn.blobFromImage(\n",
    "            frame,\n",
    "            1.0,\n",
    "            (inWidth, inHeight),\n",
    "            (127.5, 127.5, 127.5),\n",
    "            swapRB=True,\n",
    "            crop=False,\n",
    "        )\n",
    "    )\n",
    "    out = net.forward()\n",
    "    out = out[:, :19, :, :]\n",
    "\n",
    "    assert len(BODY_PARTS) == out.shape[1]\n",
    "\n",
    "    points = []\n",
    "\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        # Slice heatmap of corresponding body's part\n",
    "        heatMap = out[0, i, :, :]\n",
    "\n",
    "        # Find the global maximum in the heatmap\n",
    "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "        x = (frameWidth * point[0]) / out.shape[3]\n",
    "        y = (frameHeight * point[1]) / out.shape[2]\n",
    "        # Add a point if its confidence is higher than the threshold\n",
    "        points.append((int(x), int(y)) if conf > thr else None)\n",
    "\n",
    "    # Define body parts to keep (head, torso, arms)\n",
    "    body_parts_to_keep = [\"Nose\", \"Neck\", \"RShoulder\", \"RElbow\", \"LShoulder\", \"LElbow\"]\n",
    "\n",
    "    box_size = 125  # Half-width and half-height of the box\n",
    "\n",
    "    # Create a copy of the frame to modify\n",
    "    output_frame = frame.copy()\n",
    "\n",
    "    # Set everything initially to white\n",
    "    output_frame[:, :] = (255, 255, 255)\n",
    "\n",
    "    for part in body_parts_to_keep:\n",
    "        part_id = BODY_PARTS[part]\n",
    "\n",
    "        # Check if the keypoint exists and is valid\n",
    "        if points[part_id]:\n",
    "            x, y = points[part_id]\n",
    "\n",
    "            # Define the rectangular region for the body part\n",
    "            top_left = (max(0, x - box_size), max(0, y - box_size))\n",
    "            bottom_right = (\n",
    "                min(frameWidth, x + box_size),\n",
    "                min(frameHeight, y + box_size),\n",
    "            )\n",
    "\n",
    "            # Copy the rectangular region from the original frame to the output frame\n",
    "            output_frame[\n",
    "                top_left[1] : bottom_right[1], top_left[0] : bottom_right[0]\n",
    "            ] = frame[top_left[1] : bottom_right[1], top_left[0] : bottom_right[0]]\n",
    "\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform pose estimation\n",
    "estimated_image = pose_estimation(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the estimated image\n",
    "estimated_image = plt.imshow(estimated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the AxesImage object back to a NumPy array\n",
    "estimated_image_array = estimated_image.get_array().data\n",
    "print(estimated_image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the image\n",
    "cv.imwrite(\"extracted_body_parts.png\", estimated_image_array)\n",
    "cv.waitKey(5000)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VITON-HD Virtual Try-On Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **1. Setup the Environment**\n",
    "\n",
    "# Install dependencies\n",
    "%pip install torch torchvision numpy matplotlib opencv-python\n",
    "\n",
    "# Note: Creating and activating a virtual environment should be done in a terminal, not in a Jupyter Notebook cell.\n",
    "# The following commands are for reference and should be run in a terminal:\n",
    "# !conda create -n viton_hd python=3.7 -y\n",
    "# !conda activate viton_hd\n",
    "\n",
    "import torch\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:\n",
    "            return self.linear(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "model = MyModel()\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "x = torch.randn(1, 10)\n",
    "output = scripted_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **2. Preprocess Data**\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "\n",
    "# Dummy functions for pose estimation and segmentation\n",
    "def estimate_pose(image):\n",
    "    # Dummy implementation for pose estimation\n",
    "    return \"pose_map\"\n",
    "\n",
    "\n",
    "def segment_person(image):\n",
    "    # Dummy implementation for person segmentation\n",
    "    return \"segmentation_map\"\n",
    "\n",
    "\n",
    "# Function to preprocess person images\n",
    "def preprocess_person(image_path):\n",
    "    person_image = cv2.imread(image_path)\n",
    "    if person_image is None:\n",
    "        print(\n",
    "            f\"Person image not found at path: {image_path}. Using dummy pose and segmentation maps.\"\n",
    "        )\n",
    "        return \"pose_map\", \"segmentation_map\"\n",
    "    pose_map = estimate_pose(person_image)  # Pose estimation function\n",
    "    segmentation_map = segment_person(person_image)  # Segmentation function\n",
    "    return pose_map, segmentation_map\n",
    "\n",
    "\n",
    "# Function to preprocess clothing images\n",
    "def preprocess_clothing(image_path):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((1024, 768)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    clothing_image = cv2.imread(image_path)\n",
    "    if clothing_image is None:\n",
    "        print(f\"Clothing image not found at path: {image_path}. Using a dummy tensor.\")\n",
    "        return torch.zeros((3, 1024, 768))  # Dummy tensor with the same shape\n",
    "    clothing_image = cv2.cvtColor(clothing_image, cv2.COLOR_BGR2RGB)\n",
    "    clothing_image = transform(clothing_image)\n",
    "    return clothing_image\n",
    "\n",
    "\n",
    "# Define the paths to the person and clothing images from the input_images folder. Note: The paths should be relative to the current working directory\n",
    "\n",
    "person_image_path = \"pose1.png\"\n",
    "clothing_image_path = \"Project_3\\input_images\\Ballgown.png\"\n",
    "\n",
    "# Show image from clothing_image_path\n",
    "clothing_image = cv2.imread(clothing_image_path)\n",
    "if clothing_image is not None:\n",
    "    plt.imshow(cv2.cvtColor(clothing_image, cv2.COLOR_BGR2RGB))\n",
    "else:\n",
    "    print(f\"Clothing image not found at path: {clothing_image_path}\")\n",
    "\n",
    "# Preprocess the images\n",
    "pose_map, segmentation_map = preprocess_person(person_image_path)\n",
    "clothing_image = preprocess_clothing(clothing_image_path)\n",
    "\n",
    "# Preprocess the images\n",
    "pose_map, segmentation_map = preprocess_person(person_image_path)\n",
    "clothing_image = preprocess_clothing(clothing_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **2. Preprocess Data**\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "\n",
    "# Dummy functions for pose estimation and segmentation\n",
    "def estimate_pose(image):\n",
    "    # Dummy implementation for pose estimation\n",
    "    return \"pose_map\"\n",
    "\n",
    "\n",
    "def segment_person(image):\n",
    "    # Dummy implementation for person segmentation\n",
    "    return \"segmentation_map\"\n",
    "\n",
    "\n",
    "# Function to preprocess person images\n",
    "def preprocess_person(image_path):\n",
    "    person_image = cv2.imread(image_path)\n",
    "    if person_image is None:\n",
    "        raise FileNotFoundError(f\"Person image not found at path: {image_path}\")\n",
    "    pose_map = estimate_pose(person_image)  # Pose estimation function\n",
    "    segmentation_map = segment_person(person_image)  # Segmentation function\n",
    "    return pose_map, segmentation_map\n",
    "\n",
    "\n",
    "# Function to preprocess clothing images\n",
    "def preprocess_clothing(image_path):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((1024, 768)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    clothing_image = cv2.imread(image_path)\n",
    "    if clothing_image is None:\n",
    "        raise FileNotFoundError(f\"Clothing image not found at path: {image_path}\")\n",
    "    clothing_image = cv2.cvtColor(clothing_image, cv2.COLOR_BGR2RGB)\n",
    "    clothing_image = transform(clothing_image)\n",
    "    return clothing_image\n",
    "\n",
    "\n",
    "# Define the paths to the person and clothing images from the input_images folder\n",
    "\n",
    "person_image_path = \"Project_3\\input_images\\pose1.png\"\n",
    "clothing_image_path = \"Project_3\\input_images\\Ballgown.png\"\n",
    "\n",
    "# Show image from clothing_image_path\n",
    "clothing_image = cv2.imread(clothing_image_path)\n",
    "plt.imshow(clothing_image)\n",
    "\n",
    "# Preprocess the images\n",
    "pose_map, segmentation_map = preprocess_person(person_image_path)\n",
    "clothing_image = preprocess_clothing(clothing_image_path)\n",
    "\n",
    "try:\n",
    "    pose_map, segmentation_map = preprocess_person(person_image_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Person image not found. Using dummy pose and segmentation maps.\")\n",
    "    pose_map, segmentation_map = \"pose_map\", \"segmentation_map\"\n",
    "\n",
    "try:\n",
    "    clothing_image = preprocess_clothing(clothing_image_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Clothing image not found. Using a dummy tensor.\")\n",
    "    clothing_image = torch.zeros((3, 1024, 768))  # Dummy tensor with the same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **3. Clothing Image Warping (Align Clothes to the Person)**\n",
    "\n",
    "# # Preprocess person and clothing images # Uncomment this block if you want to run the code in this cell\n",
    "# pose_map, segmentation_map = preprocess_person(\"/content/drive/My Drive/person.jpg\")\n",
    "# person_image = preprocess_person(\"/content/drive/My Drive/person.jpg\")\n",
    "# clothing_image = preprocess_clothing(\"/content/drive/My Drive/clothing.jpg\")\n",
    "\n",
    "\n",
    "# Dummy function for warping clothing image\n",
    "def warp_clothing(clothing_image, pose_map, segmentation_map):\n",
    "    # Dummy implementation for warping clothing image\n",
    "    return \"warped_clothing\"\n",
    "\n",
    "\n",
    "# Warp the clothing image to align with the person\n",
    "warped_clothing = warp_clothing(clothing_image, pose_map, segmentation_map)\n",
    "\n",
    "# Display the warped clothing image\n",
    "plt.imshow(warped_clothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **4. Warping clothing to align with the person’s shape**\n",
    "\n",
    "%pip install scikit-image\n",
    "\n",
    "import numpy as np\n",
    "from skimage.transform import PiecewiseAffineTransform, warp\n",
    "import torch\n",
    "\n",
    "# Ensure clothing_image is defined\n",
    "try:\n",
    "    clothing_image\n",
    "except NameError:\n",
    "    raise NameError(\"clothing_image is not defined. Please ensure the cell defining clothing_image is executed.\")\n",
    "\n",
    "# Convert clothing_image to NumPy array if it's a tensor\n",
    "if isinstance(clothing_image, torch.Tensor):\n",
    "    clothing_image = clothing_image.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Define transformation points\n",
    "src_cols = np.linspace(0, clothing_image.shape[1], 10)\n",
    "src_rows = np.linspace(0, clothing_image.shape[0], 10)\n",
    "src_rows, src_cols = np.meshgrid(src_rows, src_cols)\n",
    "src = np.dstack([src_cols.flat, src_rows.flat])[0]\n",
    "\n",
    "# Destination points with slight perturbation\n",
    "dst = src + np.random.normal(0, 5, src.shape)\n",
    "\n",
    "# Apply transformation\n",
    "tform = PiecewiseAffineTransform()\n",
    "tform.estimate(src, dst)\n",
    "warped_clothing = warp(clothing_image, tform)\n",
    "\n",
    "# Convert back to tensor\n",
    "warped_clothing = torch.tensor(warped_clothing).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **5. Clothing Image Warping (Align Clothes to the Person)** ## This is alternate code for the section above; need to combine or eliminate one of them\n",
    "%pip install scikit-image\n",
    "\n",
    "import numpy as np\n",
    "from skimage.transform import PiecewiseAffineTransform, warp\n",
    "\n",
    "# Ensure clothing_image is defined\n",
    "try:\n",
    "  clothing_image\n",
    "except NameError:\n",
    "  raise NameError(\"clothing_image is not defined. Please ensure the cell defining clothing_image is executed.\")\n",
    "\n",
    "# Warping clothing to align with the person’s shape\n",
    "tform = PiecewiseAffineTransform()\n",
    "\n",
    "# Assuming pose_map is a set of control points for the transformation\n",
    "# You need to define source and destination control points for the transformation\n",
    "# Here we use dummy control points for illustration\n",
    "src_cols = np.linspace(0, clothing_image.shape[1], 10)\n",
    "src_rows = np.linspace(0, clothing_image.shape[0], 10)\n",
    "src_rows, src_cols = np.meshgrid(src_rows, src_cols)\n",
    "src = np.dstack([src_cols.flat, src_rows.flat])[0]\n",
    "\n",
    "dst = src + np.random.normal(0, 5, src.shape)\n",
    "tform.estimate(src, dst)\n",
    "\n",
    "warped_clothing = warp(clothing_image, tform)\n",
    "warped_clothing = torch.tensor(warped_clothing).permute(2, 0, 1)\n",
    "\n",
    "# Show the warped clothing image\n",
    "plt.imshow(warped_clothing.permute(1, 2, 0)) # Convert to HWC format for display\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **6. Overlaying the warped clothing onto the image**\n",
    "\n",
    "# Pick a few `src` points by hand, and move the corresponding `dst` points to their\n",
    "# expected positions. Need to align the points with the clothing image so that src and dst points match correclty.\n",
    "# fmt: off\n",
    "src = np.array([[22,  22], [100,  10], [177, 22], [190, 100], [177, 177], [100, 188],\n",
    "                [22, 177], [ 10, 100], [ 66, 66], [133,  66], [ 66, 133], [133, 133]])\n",
    "dst = np.array([[ 0,   0], [100,   0], [200,  0], [200, 100], [200, 200], [100, 200],\n",
    "                [ 0, 200], [  0, 100], [ 73, 73], [128,  73], [ 73, 128], [128, 128]])\n",
    "# fmt: on\n",
    "\n",
    "# Estimate the TPS transformation from these points and then warp the image.\n",
    "# We switch `src` and `dst` here because `skimage.transform.warp` requires the\n",
    "# inverse transformation!\n",
    "tps = ski.transform.ThinPlateSplineTransform()\n",
    "tps.estimate(dst, src)\n",
    "warped = ski.transform.warp(image, tps)\n",
    "\n",
    "# Plot the results\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(image, cmap=\"gray\")\n",
    "axs[0].scatter(src[:, 0], src[:, 1], marker=\"x\", color=\"cyan\")\n",
    "axs[1].imshow(warped, cmap=\"gray\", extent=(0, 200, 200, 0))\n",
    "axs[1].scatter(dst[:, 0], dst[:, 1], marker=\"x\", color=\"cyan\")\n",
    "\n",
    "point_labels = [str(i) for i in range(len(src))]\n",
    "for i, label in enumerate(point_labels):\n",
    "    axs[0].annotate(\n",
    "        label,\n",
    "        (src[:, 0][i], src[:, 1][i]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 5),\n",
    "        ha=\"center\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "    axs[1].annotate(\n",
    "        label,\n",
    "        (dst[:, 0][i], dst[:, 1][i]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 5),\n",
    "        ha=\"center\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **5. Train the Model (if needed)**\n",
    "\n",
    "import torch\n",
    "\n",
    "# Define dummy dataloader, adversarial_loss, perceptual_loss, and ground_truth for the example\n",
    "dataloader = [(\"person_data\", clothing_image)]  # Dummy dataloader\n",
    "adversarial_loss = lambda output, target: torch.tensor(0.0)  # Dummy loss function\n",
    "perceptual_loss = lambda output, target: torch.tensor(0.0)  # Dummy loss function\n",
    "ground_truth = torch.zeros_like(clothing_image)  # Dummy ground truth\n",
    "\n",
    "\n",
    "# Define the tryon_model architecture\n",
    "class TryOnModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TryOnModel, self).__init__()\n",
    "        # Example model architecture with convolutional layers\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=6, out_channels=64, kernel_size=4, stride=2, padding=1\n",
    "        )\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1\n",
    "        )\n",
    "        self.conv3 = torch.nn.Conv2d(\n",
    "            in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1\n",
    "        )\n",
    "        self.conv4 = torch.nn.Conv2d(\n",
    "            in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(in_features=512 * 16 * 12, out_features=3 * 64 * 48)\n",
    "\n",
    "    def forward(self, clothing_data, person_data):\n",
    "        # Example forward pass\n",
    "        x = torch.cat((clothing_data, person_data), dim=1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = torch.sigmoid(self.fc(x))\n",
    "        output = output.view(-1, 3, 64, 48)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialize the tryon_model\n",
    "tryon_model = TryOnModel()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(tryon_model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        person_data, clothing_data = batch\n",
    "        output = tryon_model(clothing_data, person_data)\n",
    "        loss = adversarial_loss(output, ground_truth) + perceptual_loss(\n",
    "            output, ground_truth\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **6. Run Inference**\n",
    "\n",
    "# Load trained model\n",
    "try:\n",
    "    tryon_model.load_state_dict(torch.load(\"viton_hd_weights.pth\"))\n",
    "    tryon_model.eval()\n",
    "except FileNotFoundError:\n",
    "    print(\"Weights file not found. Using dummy weights for testing.\")\n",
    "    dummy_state_dict = tryon_model.state_dict()\n",
    "    tryon_model.load_state_dict(dummy_state_dict)\n",
    "\n",
    "# Ensure warped_clothing is defined\n",
    "try:\n",
    "    warped_clothing\n",
    "except NameError:\n",
    "    print(\n",
    "        \"warped_clothing is not defined. Please ensure the cell generating warped_clothing is executed.\"\n",
    "    )\n",
    "    # Dummy warped_clothing for testing\n",
    "    warped_clothing = torch.zeros_like(clothing_image)\n",
    "\n",
    "# Generate try-on result\n",
    "with torch.no_grad():\n",
    "    output_image = tryon_model(warped_clothing, segmentation_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **7. Display Results**\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the output_image is defined\n",
    "try:\n",
    "    plt.imshow(output_image.permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    print(\n",
    "        \"output_image is not defined. Please ensure the cell generating output_image is executed.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Generate Try-On Image with Thin-Plate-Spline Normalization\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.interpolate import Rbf\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.interpolate import Rbf\n",
    "\n",
    "# Create your data points (x, y, z)\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "z = np.array([2, 5, 3, 8, 1])\n",
    "\n",
    "\n",
    "# Create the thin plate spline interpolator\n",
    "\n",
    "interp_function = Rbf(x, y, z, function=\"thin_plate\")\n",
    "\n",
    "\n",
    "# Evaluate the interpolated value at new points (x_new, y_new)\n",
    "\n",
    "x_new = np.array([2.5, 3.5])\n",
    "\n",
    "y_new = np.array([2.5, 3.5])\n",
    "\n",
    "z_interp = interp_function(x_new, y_new)\n",
    "\n",
    "print(z_interp)\n",
    "\n",
    "# Load images\n",
    "person_img_path = r\"C:/Users/kerim/Desktop/Project 3 Local Code/person.jpg\"  # Change to actual person image path\n",
    "cloth_img_path = r\"C:/Users/kerim/Desktop/Project 3 Local Code/clothing.jpg\"  # Change to actual clothing image path\n",
    "\n",
    "person_img = cv2.imread(person_img_path)\n",
    "cloth_img = cv2.imread(cloth_img_path)\n",
    "\n",
    "if person_img is None or cloth_img is None:\n",
    "    raise FileNotFoundError(\"One of the input images was not found.\")\n",
    "\n",
    "# Example Key Points (Adjust based on your dataset)\n",
    "person_points = np.array(\n",
    "    [\n",
    "        [100, 50],  # Shoulder Left\n",
    "        [200, 50],  # Shoulder Right\n",
    "        [120, 150],  # Waist Left\n",
    "        [180, 150],  # Waist Right\n",
    "    ]\n",
    ")\n",
    "\n",
    "cloth_points = np.array(\n",
    "    [\n",
    "        [90, 40],  # Neck Left\n",
    "        [210, 40],  # Neck Right\n",
    "        [110, 140],  # Bottom Left\n",
    "        [190, 140],  # Bottom Right\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Function for TPS Warping using SciPy\n",
    "def thin_plate_spline_warp(cloth_img, src_points, dst_points):\n",
    "    height, width = cloth_img.shape[:2]\n",
    "    grid_x, grid_y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    src_x, src_y = src_points[:, 0], src_points[:, 1]\n",
    "    dst_x, dst_y = dst_points[:, 0], dst_points[:, 1]\n",
    "\n",
    "    tps_x = Rbf(src_x, src_y, dst_x, function=\"thin_plate\")\n",
    "    tps_y = Rbf(src_x, src_y, dst_y, function=\"thin_plate\")\n",
    "\n",
    "    map_x = tps_x(grid_x, grid_y).astype(np.float32)\n",
    "    map_y = tps_y(grid_x, grid_y).astype(np.float32)\n",
    "\n",
    "    warped_cloth = cv2.remap(cloth_img, map_x, map_y, interpolation=cv2.INTER_LINEAR)\n",
    "    return warped_cloth\n",
    "\n",
    "\n",
    "# Apply TPS warping\n",
    "warped_clothing = thin_plate_spline_warp(clothing_image, cloth_points, person_points)\n",
    "\n",
    "# Save & Show Warped Clothing\n",
    "cv2.imwrite(\"warped_clothing.jpg\", warped_clothing)\n",
    "plt.imshow(cv2.cvtColor(warped_clothing, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Warped Clothing\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Blending the Warped Clothing with Person Image\n",
    "# Load person image again cv2.imread\n",
    "person_image_path = \"/content/drive/My Drive/person.jpg\"\n",
    "person_image_for_blending = cv2.imread(person_image_path)\n",
    "\n",
    "warped_clothing = cv2.resize(\n",
    "    warped_clothing,\n",
    "    (person_image_for_blending.shape[1], person_image_for_blending.shape[0]),\n",
    ")\n",
    "alpha = 0.7  # Transparency Level\n",
    "\n",
    "# Convert warped_clothing to the same data type as person_image_for_blending\n",
    "warped_clothing = warped_clothing.astype(person_image_for_blending.dtype)\n",
    "\n",
    "blended = cv2.addWeighted(\n",
    "    person_image_for_blending, 1 - alpha, warped_clothing, alpha, 0\n",
    ")\n",
    "\n",
    "# Save & Show Final Output\n",
    "cv2.imwrite(\"final_tryon.jpg\", blended)\n",
    "plt.imshow(cv2.cvtColor(blended, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Final Try-On\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Ensure warped_clothing and segmentation_map are defined\n",
    "try:\n",
    "    warped_clothing\n",
    "    segmentation_map = np.zeros_like(warped_clothing)  # dummy assignment to avoid error\n",
    "except NameError:\n",
    "    raise NameError(\n",
    "        \"warped_clothing or segmentation_map is not defined. Please ensure the cells defining them are executed.\"\n",
    "    )\n",
    "\n",
    "# Load images\n",
    "person_img_path = r\"C:/Users/kerim/Desktop/Project 3 Local Code/person.jpg\"  # Change to actual person image path\n",
    "cloth_img_path = r\"C:/Users/kerim/Desktop/Project 3 Local Code/clothing.jpg\"  # Change to actual clothing image path\n",
    "\n",
    "person_img = cv2.imread(person_img_path)\n",
    "cloth_img = cv2.imread(cloth_img_path)\n",
    "\n",
    "if person_img is None or cloth_img is None:\n",
    "    raise FileNotFoundError(\"One of the input images was not found.\")\n",
    "\n",
    "# Blending the Warped Clothing with Person Image\n",
    "warped_clothing = cv2.resize(\n",
    "    warped_clothing, (person_image.shape[1], person_image.shape[0])\n",
    ")\n",
    "alpha = 0.7  # Transparency Level\n",
    "blended = cv2.addWeighted(person_image, 1 - alpha, warped_clothing, alpha, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
